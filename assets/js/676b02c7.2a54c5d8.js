"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[9160],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>y});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),u=a,y=m["".concat(s,".").concat(u)]||m[u]||d[u]||o;return n?r.createElement(y,i(i({ref:t},c),{},{components:n})):r.createElement(y,i({ref:t},c))}));function y(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[m]="string"==typeof e?e:a,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},38600:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var r=n(87462),a=(n(67294),n(3905));const o={sidebar_position:4},i="Llama Index: Talk to your Data",l={unversionedId:"prem-app/examples/llama-index-talk-to-your-data",id:"prem-app/examples/llama-index-talk-to-your-data",title:"Llama Index: Talk to your Data",description:"- Install the necessary Dependencies",source:"@site/docs/prem-app/examples/llama-index-talk-to-your-data.md",sourceDirName:"prem-app/examples",slug:"/prem-app/examples/llama-index-talk-to-your-data",permalink:"/docs/prem-app/examples/llama-index-talk-to-your-data",draft:!1,editUrl:"https://github.com/premAI-io/dev-portal/docs/prem-app/examples/llama-index-talk-to-your-data.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Langchain: Talk to your Data",permalink:"/docs/prem-app/examples/langchain-talk-to-your-data"},next:{title:"Prem Daemon",permalink:"/docs/category/prem-daemon"}},s={},p=[],c={toc:p},m="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(m,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"llama-index-talk-to-your-data"},"Llama Index: Talk to your Data"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Install the necessary Dependencies")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import os\n\nfrom llama_index.vector_stores import RedisVectorStore\nfrom llama_index.storage.storage_context import StorageContext\nfrom llama_index import ListIndex, LLMPredictor, Document\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nfrom llama_index import LangchainEmbedding, ServiceContext\n\nos.environ["OPENAI_API_KEY"] = "random-string"\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Create the Documents using Prem Landing Page Content")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'doc1 = Document(text="Prem is an easy to use open source AI platform. With Prem you can quickly build privacy preserving AI applications.")\ndoc2 = Document(text="""\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n""")\ndoc3 = Document(text="""\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI\'s API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem\'s got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It\'s a permissionless infrastructure, designed for you.\n""")\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Instantiate the LLMs objects accordingly")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'# Using vicuna-7b-q4\nllm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_base="http://localhost:8111/api/v1", max_tokens=128))\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base="http://localhost:8444/api/v1")\n\nembed_model = LangchainEmbedding(embeddings)\nservice_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\n\nvector_store = RedisVectorStore(\n    index_name="prem_landing",\n    index_prefix="llama",\n    redis_url="redis://localhost:6379",\n    overwrite=True\n)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = ListIndex.from_documents([doc1, doc2, doc3], storage_context=storage_context)\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Query the Index")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'query_engine = index.as_query_engine(\n    retriever_mode="embedding", \n    verbose=True, \n    service_context=service_context\n)\nresponse = query_engine.query("What are Prem benefits?")\nprint(response)\n')))}d.isMDXComponent=!0}}]);